{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio 1\n",
    "### Inciso a\n",
    "Probar que $\\mathbb{1}$, el vector de $n$ unos, es autovector de $R$ y $L$, dar su autovalor, y explicar qué agrupación de la red representa ese autovector.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos primero que es autovector de $L$.\n",
    "Veamos que \n",
    "$$L_{ij} = \\begin{cases}\n",
    "        \\sum_{h=1}^{n}{A_{ih}} - A_{ij} & \\text{si } i=j \\\\ \n",
    "        -A_{ij} & \\text{si } i\\neq j \n",
    "\\end{cases}$$\n",
    "\n",
    "Reducimos esta expresion, usando que $A_{ii} = 0$, para cualquier $i$.\n",
    "\n",
    "$$L_{ij} = \\begin{cases}\n",
    "        \\sum_{h=1}^{n}{A_{ih}} & \\text{si } i=j \\\\ \n",
    "        -A_{ij} & \\text{si } i\\neq j \n",
    "\\end{cases}$$\n",
    "\n",
    "Veamos que cumple la ecuacion\n",
    "\n",
    "$$L \\cdot \\mathbb{1} = \\lambda_1 \\cdot \\mathbb{1}$$\n",
    "\n",
    "Esto es lo mismo que ver un sistema de ecuaciones de $n$ ecuaciones. Miremos una arbitraria, con el indice $k$\n",
    "\n",
    "$$\\sum_{i=1}^{n} L_{ki} = \\lambda_1$$\n",
    "\n",
    "$$\\sum_{i=1 \\land i\\neq k}^{n} L_{ki} + L_{kk} = \\lambda_1$$\n",
    "\n",
    "$$\\sum_{i=1 \\land i\\neq k}^{n}{-A_{ki}} + \\sum_{h=1}^{n}{A_{kh}} = \\lambda_1$$\n",
    "\n",
    "$$\\sum_{h=1}^{n}{A_{kh}} = \\lambda_1 + \\sum_{i=1 \\land i\\neq k}^{n}{A_{ki}}$$\n",
    "\n",
    "Si hacemos un cambio de indices, vemos que de un lado nos queda solo el $kk$ y del otro $\\lambda_1$\n",
    "\n",
    "$$\\sum_{h=1}^{n}{A_{kh}} = \\lambda_1 + \\sum_{h=1 \\land h\\neq k}^{n}{A_{ki}}$$\n",
    "\n",
    "$$A_{kk} = \\lambda_1$$\n",
    "\n",
    "Luego, vemos que, como esta era una ecuacion genérica, esto vale para cualquier $k$. Como sabemos que la diagonal de la matriz de adyacencias $A$ es 0 (pues nunca un museo se conecta consigo mismo), concluimos que:\n",
    "\n",
    "$$\\lambda_1 = 0$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos lo mismo para la matriz $R = A - P$, donde $P_{ij} = \\frac{k_i \\cdot k_j}{2E}$. \n",
    "En esta ecuacion, E es la cantidad total de aristas, es decir, $2E = \\sum_{i=1}^{n}{\\sum_{j=1}^{n}{A_{ij}}}$.\n",
    "Ademas, $k_i = K_{ii} = \\sum_{j=1}^{n}{A_{ij}}$\n",
    "\n",
    "Veamos entonces la demostracion de que $\\mathbb{1}$ es autovector de $R$, y con qué autovalor. Lo llamamos $\\lambda_2$.\n",
    "\n",
    "$$R \\cdot\\mathbb{1} = \\lambda_2 \\cdot \\mathbb{1}$$\n",
    "\n",
    "Vamos a usar la misma idea de la demostración anterior. Podemos escribir esto como un sistema de $n$ ecuaciones, y tomamos una genérica, la $k$-ésima\n",
    "\n",
    "$$\n",
    "\\sum_{j=1}^{n}{R_{kj}} = \\lambda_2\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\sum_{j=1}^{n}{A_{kj} - \\frac{k_k \\cdot k_j}{2E}} = \\lambda_2\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\sum_{j=1}^{n}{A_{kj} = \\lambda_2 + \\sum_{j=1}^{n}{\\frac{k_k \\cdot k_j}{2E}}} \n",
    "$$\n",
    "\n",
    "$$\n",
    "\\sum_{j=1}^{n}{A_{kj} = \\lambda_2 + k_k \\cdot \\sum_{j=1}^{n}{k_j}{2E}} \n",
    "$$\n",
    "\n",
    "Recordamos que $k_k = \\sum_{h=1}^{n}{A_{kh}}$. Luego:\n",
    "\n",
    "$$\n",
    "\\sum_{j=1}^{n}{A_{kj} = \\lambda_2 + (\\sum_{h=1}^{n}{A_{kh}}) \\cdot \\sum_{j=1}^{n}{\\frac{\\sum_{h=1}^{n}{A_{jh}}}\n",
    "{2E}}} \n",
    "$$\n",
    "\n",
    "$$\n",
    "\\sum_{j=1}^{n}{A_{kj} = \\lambda_2 + \\frac{\\sum_{h=1}^{n}{A_{kh}}}{2E}\\sum_{j=1}^{n}({\\sum_{h=1}^{n}{A_{jh}}})} \n",
    "$$\n",
    "\n",
    "$$\n",
    "\\sum_{j=1}^{n}{A_{kj} = \\lambda_2 + \\frac{\\sum_{h=1}^{n}{A_{kh}}}{2E} \\sum_{j=1}^{n}\\sum_{h=1}^{n}{A_{jh}}}\n",
    "$$\n",
    "\n",
    "Recordemos que $2E = \\sum_{i=1}^{n}{\\sum_{j=1}^{n}{A_{ij}}}$. Luego:\n",
    "\n",
    "$$\n",
    "\\sum_{j=1}^{n}{A_{kj} = \\lambda_2 + \\sum_{h=1}^{n}{A_{kh}}}\n",
    "$$\n",
    "\n",
    "Si renombramos el indice $h$ al indice $j$, vemos que nos queda:\n",
    "\n",
    "$$\\lambda_2 = 0$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego, queda demostrado que el vector $\\mathbb{1}$ es autovector de las matrices $R$ y $L$, con el mismo autovalor $\\lambda_1 = \\lambda_2 = 0$.\n",
    "\n",
    "La interpretacion del autovector $\\mathbb{1}$ es la asignación de todos los museos al mismo grupo, ya que la asignación a un grupo o a otro es con $1$ o $-1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inciso b\n",
    "Usamos la sugerencia del enunciado, primero demostrando que, para una matriz simétrica arbitraria $M$, con dos autovectores $v1$, $v2$ con $\\lambda_1 \\neq \\lambda_2$ autovalores asociados. Veamos que ambas ecuaciones, $v_1^t M v_2$ y $v_2^t M v_1$ son un escalar, por lo tanto su transpuesta es la misma. Esto lo podemos verificar asi, sabiendo que M es simétrica.\n",
    "\n",
    "$$v_1^t M v_2 = v_2^t M^t v_1 = v_2^t M v_1$$ \n",
    "\n",
    "Luego, ambas expresiones son el mismo escalar.\n",
    "\n",
    "$$v_1^t M v_2 = v_2^t M v_1$$\n",
    "\n",
    "$$v_1^t \\lambda_2 v_2 = v_2^t \\lambda_1 v_1$$\n",
    "\n",
    "$$\\lambda_2 v_1^t v_2 = \\lambda_1 v_2^t  v_1$$\n",
    "\n",
    "$$\\lambda_2 v_1^t v_2 - \\lambda_1 v_2^t  v_1 = 0$$\n",
    "\n",
    "Usamos que $v_1^t v_2 = v_2^t v_1$\n",
    "\n",
    "$$(\\lambda_2 - \\lambda_1)(v_1^t v_2) = 0$$\n",
    "\n",
    "Luego, alguno de los dos terminos tiene que ser cero. \n",
    "\n",
    "Como ya dijimos que los autovalores son distintos, no queda otra que concluir.\n",
    "\n",
    "$$v_1^t v_2 = 0$$\n",
    "\n",
    "Luego, tenemos que verificar que esto vale para las matrices $L$ y $R$. \n",
    "Quiero ver que $L^t = L$\n",
    "\n",
    "$$L^t = K^t - A^t = A$$\n",
    "\n",
    "Esto vale trivialmente puesto que $A$ es simétrica y $K$ es diagonal.\n",
    "\n",
    "Quiero ver que $R^t = R$\n",
    "\n",
    "$$R^t = A^t - P^t$$\n",
    "\n",
    "Como ya vimos $A$ es simétrica, queda ver que lo sea $P$. Queremos ver que:\n",
    "\n",
    "$$P_{ij} = P_{ji}$$\n",
    "\n",
    "$$\\frac{k_i \\cdot k_j}{2E} = \\frac{k_j \\cdot k_i}{2E} $$\n",
    "\n",
    "Lo cual es trivialmente cierto. Luego, queda demostrado que la propiedad vale para $R$ y $L$.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inciso c\n",
    "Para este ejercicio vamos a usar la propiedad recién demostrada. Tomamos una matriz simétrica $M$ que puede ser $R$ o $L$. También usamos que $\\mathbb{1}$ es autovector de $R$ y de $L$ con autovalor 0. Como sabemos que el autovalor de $v \\neq 0$, podemos usar la propiedad de que el producto de dos vectores con autovalores distintos de una matriz simétrica es igual a 0. \n",
    "\n",
    "$$v_1^t v_2 = 0$$\n",
    "\n",
    "\n",
    "En este caso particular, la ecuación queda así.\n",
    "\n",
    "$$v \\cdot \\mathbb{1} = 0$$\n",
    "\n",
    "$$v_1 + v_2 + \\dots + v_n = 0$$\n",
    "\n",
    "$$\\sum_{i=1}^{n}{v_i} = 0$$\n",
    "\n",
    "Tal como se quería demostrar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inciso a\n",
    "\n",
    "Mostramos que\n",
    "1) autovalores de $M + \\mu I$ son $\\gamma_i = \\lambda_i + \\mu$\n",
    "2) El autovector asociado a $\\gamma_i$ es $v_i$\n",
    "3) Si $\\mu + \\lambda_i \\neq 0 \\forall i$, entonces $M + \\mu I$ es inversible\n",
    "\n",
    "Sabemos que, para $v_i \\neq 0$, por ser autovector, vale que\n",
    "\n",
    "$$M v_i = \\lambda_i v_i$$\n",
    "\n",
    "Luego, desarollemos la siguiente expresion\n",
    "\n",
    "$$(M + \\mu I) v_i = \\lambda_i v_i + \\mu v_i = (\\lambda_i + \\mu) v_i$$\n",
    "\n",
    "Luego, si tomamos $\\gamma_i = \\lambda_i + \\mu$, vemos que\n",
    "\n",
    "$$(M + \\mu I) v_i = \\gamma_i v_i$$\n",
    "\n",
    "Luego, $v_i$ es autovector de $M + \\mu I$ con autovalor $\\gamma_i$. Quedan demostrados los puntos 1 y 2.\n",
    "\n",
    "Veamos el punto 3. Tomamos $i$ arbitraria. Queremos ver que\n",
    "\n",
    "$$\\mu + \\lambda_i \\neq 0 \\implies M + \\mu I \\text{ es inversible}$$\n",
    "\n",
    "$$\\gamma_i \\neq 0 \\implies M + \\mu I \\text{ es inversible}$$\n",
    "\n",
    "Luego, esto vale por la propiedad de que una matriz es inversible si y solo si todos sus autovalores son distintos de 0. Luego, si $\\gamma_i \\neq 0 \\forall i$, entonces $M + \\mu I$ es inversible.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inciso b\n",
    "\n",
    "#### Parte 1\n",
    "Para $\\mu \\gt 0$, mostrar que $L + \\mu I$ es inversible, sabiendo que los autovalores de $L$ son no negativos. Recordamos tambien que $L = K - A$, con $K_{ii} = \\sum_{j=1}^{n}{A_{ij}}$\n",
    "\n",
    "Usando el inciso anterior, vemos que podemos definir los autovalores de $L + \\mu I$ como $\\gamma_i = \\lambda_i + \\mu$, siendo $\\lambda_i$ el i-esimo autovalor de $L$ (ordenados descendientemente).\n",
    "\n",
    "Luego, sabemos que $\\mu \\gt 0$ y $\\lambda_i \\geq 0$. Luego, $\\gamma_i \\gt 0$. Entonces, como $L + \\mu I$ tiene todos sus autovalores positivos, es inversible.\n",
    "\n",
    "#### Parte 2\n",
    "Mostrar que aplicar el metodo de la potencia a $(L + \\mu I)^{-1}$ converge al autovector de $L$ asociado a su autovalor mas chico si se parte de una semilla adecuada. \n",
    "\n",
    "Veamos que, en general, para una matriz $A$ inversible y diagonalizable, vale que $A v_i = \\lambda_i v_i$. Luego, vale que $A^{-1} v_i = \\frac{1}{\\lambda_i} v_i$.\n",
    "\n",
    "Ya sabemos que $L + \\mu I$ es inversible, por el inciso anterior. Para ver que es diagonalizable, usamos el teorema espectral, que nos dice que por ser simétrica real, es diagonalizable. Esto lo vemos usando que $L$ es simetrica real. Luego, $L + \\mu I$ tambien lo es. Luego, $L + \\mu I$ es diagonalizable. Finalmente, vemos que los autovectores de $L + \\mu I$ son los mismos que los de $L$, con autovalores asociados iguales a los de $L$ mas $\\mu$. Entonces, por lo que vimos recién, los autovectores de $(L + \\mu I)^{-1}$ son los mismos que los de $L$, con autovalores $\\frac{1}{\\lambda_i + \\mu}$.\n",
    "\n",
    "Veamos los autovalores de $L$ con un orden particular. Supongamos que $\\lambda_1 \\geq \\lambda_2 \\geq \\dots \\geq \\lambda_n \\geq 0$.\n",
    "\n",
    "Tomemos $A = (L + \\mu I)^{-1}$.\n",
    "\n",
    "Sabiendo que, para una matriz $A$, el metodo de la potencia converge al autovector de $A$ asociado al mayor autovalor (son todos positivos asi que no importa el modulo aca), vemos que, si partimos de una semilla adecuada, converge al autovector de $A$ asociado al mayor autovalor, que es $\\frac{1}{\\lambda_n + \\mu}$, ya que para maximizar esa expresion, minimizamos el denominador. Y el minimo es $\\lambda_n$, que justamente es el autovalor mas chico de $L$.\n",
    "\n",
    "Las condiciones de \"semilla adecuada\" refieren a que el vector inicial para el metodo de la potencia no sea ortogonal a $v_n$.\n",
    "\n",
    "El caso de que haya solamente un autovector asociado al autovalor mas chico sucede cuando la multiplicidad de ese autovalor es 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inciso c\n",
    "Tenemos una matriz $M \\in \\mathbb{R}^{n \\times n}$ simétrica, y queremos ver que $M - \\lambda_1 \\frac{v_1 v_1^t}{v_1^t v_1}$ tiene los mismos autovectores que $M$, pero el autovalor asociado a $v_1$ es 0.\n",
    "\n",
    "Como $M$ es simétrica real, admite una b.o.n de autovectores. Por lo tanto, vemos que la expresión $v_1^t v_1$ es lo mismo que $||v_1||_2^2 = 1$, pues $v_1$ pertenece a una b.o.n. Luego, escribimos directamente la expresión $M - \\lambda_1 v_1 v_1^t$.\n",
    "\n",
    "Veamos que se cumple lo que pide el ejercicio\n",
    "\n",
    "$$(M - \\lambda_1 v_1 v_1^t)  v_i = \\gamma_i v_i$$\n",
    "\n",
    "Separamos en casos. Primero vemos $i=0$ y luego $i>0$\n",
    "\n",
    "$$(M - \\lambda_1 v_1 v_1^t)  v_1 = \\gamma_1 v_1$$\n",
    "\n",
    "$$M  v_1 - \\lambda_1 v_1 v_1^t  v_1 = \\gamma_1 v_1$$\n",
    "\n",
    "$$ \\lambda_1 v_1 - \\lambda_1 v_1 = \\gamma_1 v_1$$\n",
    "\n",
    "$$0 = \\gamma_1 v_1$$\n",
    "\n",
    "Luego, vemos que $\\gamma_1 = 0$, pues $v_1 \\neq 0$ por ser autovector. Veamos $i>0$\n",
    "\n",
    "$$(M - \\lambda_1 v_1 v_1^t) v_i = \\gamma_i v_i$$\n",
    "\n",
    "$$M v_i - \\lambda_1 v_1 v_1^t v_i = \\gamma_i v_i$$\n",
    "\n",
    "$$\\lambda_i v_i - \\lambda_1 v_1 (v_1^t v_i) = \\gamma_i v_i$$\n",
    "\n",
    "Vemos que $v_1^t v_i = 0$, pues $v_1$ y $v_i$ son autovectores distintos, que pertenecen a una b.o.n.\n",
    "\n",
    "Luego, queda que:\n",
    "\n",
    "$$\\lambda_i v_i = \\gamma_i v_i$$\n",
    "\n",
    "$$\\gamma_i = \\lambda_i$$\n",
    "\n",
    "Entonces, vale que los autovectores de $M$ son los mismos que los de $M - \\lambda_1 v_1 v_1^t$, con autovalores asociados iguales a los de $M$ menos $\\lambda_1$, que es lo que queríamos demostrar.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Haciendo uso de la red de museos vimos en la primera parte que nos interesaba ver de que manera se “pesaba” la importancia de cada museo, lo cual se medía en cantidad de visitas, \n",
    "\n",
    "esto a su vez reflejaba de que forma los museos vecinos se iban a ver  impactados por el peso del museo en cuestión y viceversa.  Para formalizar este concepto se hizo uso de un \n",
    "\n",
    "ranking (pagerank) que media con que probabilidad los visitantes podian llegar a ir a un determinado museo dependiendo de dónde hubieran ido anteriormente. Para el calculo del \n",
    "\n",
    "ranking también se tuvo en cuenta un factor α que media la probabilidad de que se visite un determinado museo “lejos” del circuito en el cual se estaba moviendo el visitante. Se \n",
    "\n",
    "pudo observar que si modificabamos la cantidad de vecinos (los museos más cercanos) o el factor α esto impactaba en el ranking. En particular si aumentabamos la cantidad de museos \n",
    "\n",
    "cernanos el ranking del analizado aumentaba, y en el caso de aumentar  α  esto hacia que el ranking en general se diversifique por lo que se podria decir que el peso de un museo \n",
    "\n",
    "céntrico no aumentaba o bien que otros lo podian emparejar. De esta forma, se pudo concluir que cuanto más vecinos tenia un determinado museo esto hacía que sea más probable que sea \n",
    "\n",
    "visitado, en consecuencia su ranking aumentaba. Por otra parte, tambien pudimos ver que para hacer estimaciones de datos como la cantidad total de visitantes, si se tiene una matriz \n",
    "\n",
    "de transicion bien condicionada, se podia dar una buena estimación aún cuando hubiera errores en la informacion acerca de los visitantes que ingresaron a la red. \n",
    "\n",
    "Para la segunda parte viendo que con el ranking de alguna manera se generaba una particion de museos por puntaje, nos intersaba ver de que forma podiamos agrupar los museos para \n",
    "\n",
    "formar comunidades. En particular para este caso se buscaba biseccionar la red. Para ello se usaron dos enfoques: El corte minimo y la modularidad, en simples palabras el primero \n",
    "\n",
    "particionaba por los grupos que menor cantidad de conexiones sean necesarios remover para biseccionar y el segundo se basa en generar bisecciones iterativamente y quedarse con la \n",
    "\n",
    "que mayor modularidad genere. En la práctica se buscaba obtener un vector oṕtimo que devuelva a qué comunidad corresponde cada museo. Para el calulo de dichos vectores se observó \n",
    "\n",
    "que por medio de la busqueda de los autovalores, en el caso del corte minimo buscando el segundo autovalor mas chico de la matriz laplaciana se podia calcular un autovector que \n",
    "\n",
    "minimizaba la cantidad de conexiones lo cual derivaba en el vector óptimo para este criterio, y para la modularidad se buscaba el autovalor (positivo) más grande de la matriz R cuyo \n",
    "\n",
    "autovector  maximizaba la modularidad y daba el vector óptimo.  "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
